# On ne garde que les variables de températures mensuelles
temperature <- temperature[, 1:12]
# Suppression des variables non numériques
dataToCluster <- Filter(is.numeric, dataToCluster)
# Centrage des variables
dataToCluster <- scale(dataToCluster)
head(dataToCluster)
# -------------------------------------------------
# Algo du K-means
# -------------------------------------------------
nb_cluster_max <- 8
resKmeans <- list()              # va contenir les resultats des k-means pour chaque valeur de K
CPtheta <- rep(0,nb_cluster_max) # va contenir l'inertie intra pour chaque valeur de K
# Pour K allant de 1 a 8 (nombre de clusters)
for (K in 1:nb_cluster_max){
resKmeans[[K]] <- kmeans(dataToCluster, K, nstart = 50)
CPtheta[K] <- resKmeans[[K]]$tot.withinss
}
# on voit qu en dessous de 3 clusters l'inertie intra augmente significativement (coude)
plot(x= 1:nb_cluster_max,
y= CPtheta,
type = "b",
xlab = "K",
ylab = "Inertie intra",
main = "Inertie intra en fonction du nombre de clusters")
# Nombre d'éléments dans chacun des 3 clusters
resKmeans[[3]]$size
# Ajout de la variable qualitative cluster
cluster_kmeans <- resKmeans[[3]]$cluster
temperature <- cbind(dataToCluster, cluster_kmeans)
# Visulisation des 3 clusters sur une ACP
acp_kmeans <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_kmeans'),
graph = FALSE)
plot.PCA(acp_kmeans,
choix = "ind",
habillage = "cluster_kmeans",
col.hab = c("blue","red","green"),
invisible = c("quali"),
title = "Clustering par K-means")
temperature <- read.table("data/temperatures.csv",
header = TRUE,
sep = ";",
row.names = 1)
# on transforme la variable Region est factor
temperature$Region <- as.factor(temperature$Region)
str(temperature)
# On ne garde que les variables de températures mensuelles
temperature <- temperature[, 1:12]
# Suppression des variables non numériques
dataToCluster <- Filter(is.numeric, dataToCluster)
# Centrage des variables
dataToCluster <- scale(dataToCluster)
# Suppression des variables non numériques
dataToCluster <- Filter(is.numeric, temperature)
# Centrage des variables
dataToCluster <- scale(dataToCluster)
head(dataToCluster)
nb_cluster_max <- 8
resKmeans <- list()              # va contenir les resultats des k-means pour chaque valeur de K
CPtheta <- rep(0,nb_cluster_max) # va contenir l'inertie intra pour chaque valeur de K
# Pour K allant de 1 a 8 (nombre de clusters)
for (K in 1:nb_cluster_max){
resKmeans[[K]] <- kmeans(dataToCluster, K, nstart = 50)
CPtheta[K] <- resKmeans[[K]]$tot.withinss
}
# on voit qu en dessous de 3 clusters l'inertie intra augmente significativement (coude)
plot(x= 1:nb_cluster_max,
y= CPtheta,
type = "b",
xlab = "K",
ylab = "Inertie intra",
main = "Inertie intra en fonction du nombre de clusters")
# Nombre d'éléments dans chacun des 3 clusters
resKmeans[[3]]$size
# Ajout de la variable qualitative cluster
cluster_kmeans <- resKmeans[[3]]$cluster
temperature <- cbind(dataToCluster, cluster_kmeans)
# Visulisation des 3 clusters sur une ACP
acp_kmeans <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_kmeans'),
graph = FALSE)
plot.PCA(acp_kmeans,
choix = "ind",
habillage = "cluster_kmeans",
col.hab = c("blue","red","green"),
invisible = c("quali"),
title = "Clustering par K-means")
str(temperature)
View(temperature)
temperature <- cbind(temperature, cluster_kmeans)
# Visulisation des 3 clusters sur une ACP
acp_kmeans <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_kmeans'),
graph = FALSE)
plot.PCA(acp_kmeans,
choix = "ind",
habillage = "cluster_kmeans",
col.hab = c("blue","red","green"),
invisible = c("quali"),
title = "Clustering par K-means")
str(temperature)
temperature$cluster_kmeans
rm(list=ls())
setwd("P:/Ludo/Tuto/R-tuto")
temperature <- read.table("data/temperatures.csv",
header = TRUE,
sep = ";",
row.names = 1)
# on transforme la variable Region est factor
temperature$Region <- as.factor(temperature$Region)
library(FactoMineR)
# on transforme la variable Region est factor
temperature$Region <- as.factor(temperature$Region)
str(temperature)
# On ne garde que les variables de températures mensuelles
temperature <- temperature[, 1:12]
# Suppression des variables non numériques
dataToCluster <- Filter(is.numeric, temperature)
# Centrage des variables
dataToCluster <- scale(dataToCluster)
head(dataToCluster)
nb_cluster_max <- 8
resKmeans <- list()              # va contenir les resultats des k-means pour chaque valeur de K
CPtheta <- rep(0,nb_cluster_max) # va contenir l'inertie intra pour chaque valeur de K
# Pour K allant de 1 a 8 (nombre de clusters)
for (K in 1:nb_cluster_max){
resKmeans[[K]] <- kmeans(dataToCluster, K, nstart = 50)
CPtheta[K] <- resKmeans[[K]]$tot.withinss
}
# on voit qu en dessous de 3 clusters l'inertie intra augmente significativement (coude)
plot(x= 1:nb_cluster_max,
y= CPtheta,
type = "b",
xlab = "K",
ylab = "Inertie intra",
main = "Inertie intra en fonction du nombre de clusters")
# Nombre d'éléments dans chacun des 3 clusters
resKmeans[[3]]$size
# Ajout de la variable qualitative cluster
cluster_kmeans <- resKmeans[[3]]$cluster
temperature <- cbind(temperature, cluster_kmeans)
# Visulisation des 3 clusters sur une ACP
acp_kmeans <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_kmeans'),
graph = FALSE)
plot.PCA(acp_kmeans,
choix = "ind",
habillage = "cluster_kmeans",
col.hab = c("blue","red","green"),
invisible = c("quali"),
title = "Clustering par K-means")
str(temperature)
temperature$cluster_kmeans <- as.factor(temperature$cluster_kmeans)
plot.PCA(acp_kmeans,
choix = "ind",
habillage = "cluster_kmeans",
col.hab = c("blue","red","green"),
invisible = c("quali"),
title = "Clustering par K-means")
# Visulisation des 3 clusters sur une ACP
acp_kmeans <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_kmeans'),
graph = FALSE)
plot.PCA(acp_kmeans,
choix = "ind",
habillage = "cluster_kmeans",
col.hab = c("blue","red","green"),
invisible = c("quali"),
title = "Clustering par K-means")
temperature <- cbind(temperature, as.factor(cluster_kmeans))
# Visulisation des 3 clusters sur une ACP
acp_kmeans <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_kmeans'),
graph = FALSE)
rm(list=ls())
setwd("P:/Ludo/Tuto/R-tuto")
temperature <- read.table("data/temperatures.csv",
header = TRUE,
sep = ";",
row.names = 1)
# on transforme la variable Region est factor
temperature$Region <- as.factor(temperature$Region)
str(temperature)
# On ne garde que les variables de températures mensuelles
temperature <- temperature[, 1:12]
# Suppression des variables non numériques
dataToCluster <- Filter(is.numeric, temperature)
# Centrage des variables
dataToCluster <- scale(dataToCluster)
head(dataToCluster)
nb_cluster_max <- 8
resKmeans <- list()              # va contenir les resultats des k-means pour chaque valeur de K
CPtheta <- rep(0,nb_cluster_max) # va contenir l'inertie intra pour chaque valeur de K
# Pour K allant de 1 a 8 (nombre de clusters)
for (K in 1:nb_cluster_max){
resKmeans[[K]] <- kmeans(dataToCluster, K, nstart = 50)
CPtheta[K] <- resKmeans[[K]]$tot.withinss
}
# on voit qu en dessous de 3 clusters l'inertie intra augmente significativement (coude)
plot(x= 1:nb_cluster_max,
y= CPtheta,
type = "b",
xlab = "K",
ylab = "Inertie intra",
main = "Inertie intra en fonction du nombre de clusters")
# Nombre d'éléments dans chacun des 3 clusters
resKmeans[[3]]$size
# Ajout de la variable qualitative cluster
cluster_kmeans <- resKmeans[[3]]$cluster
temperature <- cbind(temperature, as.factor(cluster_kmeans))
# Visulisation des 3 clusters sur une ACP
acp_kmeans <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_kmeans'),
graph = FALSE)
temperature
temperature <- cbind(temperature, cluster_kmeans = as.factor(cluster_kmeans))
temperature
# On ne garde que les variables de températures mensuelles
temperature <- temperature[, 1:12]
temperature <- cbind(temperature, cluster_kmeans = as.factor(cluster_kmeans))
temperature
# Visulisation des 3 clusters sur une ACP
acp_kmeans <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_kmeans'),
graph = FALSE)
plot.PCA(acp_kmeans,
choix = "ind",
habillage = "cluster_kmeans",
col.hab = c("blue","red","green"),
invisible = c("quali"),
title = "Clustering par K-means")
# CAH avec critere de Ward
#    method = "single" pour mini
CAHward <- agnes(dataToCluster, metric = "euclidean", method = "ward")
library(cluster)
# CAH avec critere de Ward
#    method = "single" pour mini
CAHward <- agnes(dataToCluster, metric = "euclidean", method = "ward")
plot(CAHward,
which.plots = 2,
main = "Dendogramme Ward")
# Ajout de la variable qualitative cluster
cluster_cah <- cutree(CAHward, 3)
matrice <- cbind(matrice, cluster_cah)
temperature <- cbind(temperature, cluster_cah = as.factor(cluster_cah))
# Visulisation des 3 clusters sur une ACP
acp_cah <- PCA(matrice,
quali.sup = which(colnames(matrice) == 'cluster_cah'),
graph = FALSE)
# Visulisation des 3 clusters sur une ACP
acp_cah <- PCA(temperature,
quali.sup = which(colnames(matrice) == 'cluster_cah'),
graph = FALSE)
# Visulisation des 3 clusters sur une ACP
acp_cah <- PCA(temperature,
quali.sup = which(colnames(temperature) == 'cluster_cah'),
graph = FALSE)
temperature
# Visulisation des 3 clusters sur une ACP
acp_cah <- PCA(temperature,
quali.sup = which(colnames(temperature) in c("cluster_cah","cluster_kmeans")),
graph = FALSE)
# Visulisation des 3 clusters sur une ACP
acp_cah <- PCA(temperature,
quali.sup = which(colnames(temperature) %in% c("cluster_cah","cluster_kmeans")),
graph = FALSE)
plot.PCA(acp_cah,
choix = "ind",
habillage = "cluster_cah",
invisible = c("quali"),
title = "Clustering par CAH")
partitionCAH <- cutree(CAHward, 3)
partitionKmeans <- kmeans(dataToCluster, 3)
# Matrice de confusion entre les deux partitions
table(partitionKmeans$cluster, partitionCAH)
# Adjusted Rand Index (Indice de Rand)
ARI(partitionKmeans$cluster, partitionCAH)
library(VarSelLCM)
rm(list=ls())
setwd("P:/Ludo/Tuto/R-tuto")
mu0 <- 140
v0 <- 5
curve(dnorm(x, mean = mu0, sd = v0),
from = mu0 - 5 * v0,
to = mu0 + 5 * v0,
bty = "n",
main = "Densité de la loi N(140, 5)")
x <- c(133, 152, 170, 166, 155, 142, 139, 145, 149, 142, 165, 135, 150, 144, 148)
n <- length(x)
x_bar <- mean(x)
sigma0 <- 30
mu_post <- (mu0 * (sigma0 / n)  + x_bar * v0) / ((sigma0 / n) + v0)
v_post <- sigma0 * v0 / (sigma0 * n + v0)
# La postériore est donc
curve(dnorm(x, mean = mu_post, sd = v_post),
col = "red",
add = TRUE)
# Ainsi la taille moyenne après prise en compte des données vaut
mu_post
# Intervalle de crédibilité
c(qnorm(0.025, mu_post, v_post),
qnorm(0.975, mu_post, v_post))
v0_bis <- 1
mu_post_bis <- (mu0 * (sigma0 / n)  + x_bar * v0_bis) / ((sigma0 / n) + v0_bis)
mu_post_bis
curve(dnorm(x, mean = mu0, sd = v0),
from = 120, to = 160,
ylim = c(0, 0.5),
col = "purple",
bty = "n",
main = "Comparaison de lois à priori")
curve(dnorm(x, mean = mu0, sd = v0_bis),
col = "red",
add = TRUE)
curve(dnorm(x, mean = mu0, sd = v0),
from = 120, to = 160,
ylim = c(0, 0.5),
col = "purple",
bty = "n",
main = "Comparaison de lois à priori")
curve(dnorm(x, mean = mu0, sd = v0_bis),
col = "red",
add = TRUE)
rug(mu_post, col = "purple", lwd = 2, ticksize = 0.3, lty = "dashed")
rug(mu_post_bis, col = "red", lwd = 2, ticksize = 0.3, lty = "dashed")
legend("topright", inset = .05, lty = c(1, 1, 2),
c("Priore peu informative", "Priore informative", "Moyennes à postériori"),
col = c("purple", "red", "black"))
# x0 : état initial de la chaine
# L : longeur de la chaine
# s : paramètre
chaine_mcmc <- function(x0, L, s){
res <- rep(NA, L)
res[1] <- x0
for (l in 2:L){
x_old <- res[l-1]
x_new <- runif(1, min = x_old - 2 * s, max = x_old + 2 * s)    # générer un candidat
u_new <- runif(1)
rho <- pmin(1, dnorm(x_new) / dnorm(x_old))
if (u_new < rho){               # on accepte le nouveau candidat
res[l] <- x_new
}
else{                           # on conserve l'ancienne valeur
res[l] <- x_old
}
}
res
}
# Exemple
a <- chaine_mcmc(x0 = -10, L = 1600, s = 0.1)
hist(a,breaks = 100)
# Exemple
a <- chaine_mcmc(x0 = -10, L = 1600, s = 0.8)
hist(a, breaks = 100)
plot(st(a))
plot(ts(a))
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 2)
plot(ts(a))
# Cette fonction crée une chaine de Markov qui a pour loi stationnaire
# la loi Normale Centrée Réduite
#   x0 : état initial de la chaine
#   L : longeur de la chaine
#   s : paramètre
chaine_mcmc_N01 <- function(x0, L, s){
res <- rep(NA, L)
res[1] <- x0
for (l in 2:L){
x_old <- res[l-1]
x_new <- runif(1, min = x_old - 2 * s, max = x_old + 2 * s)    # générer un candidat
u_new <- runif(1)
rho <- pmin(1, dnorm(x_new) / dnorm(x_old))
if (u_new < rho){               # on accepte le nouveau candidat
res[l] <- x_new
}
else{                           # on conserve l'ancienne valeur
res[l] <- x_old
}
}
res
}
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 2)
plot(ts(a))
hist(a, breaks = 100)
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 5)
plot(ts(a))
hist(a, breaks = 100)
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 0.1)
plot(ts(a))
hist(a, breaks = 100)
plot(ts(a))
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 0.8)
plot(ts(a))
hist(a, breaks = 100)
set.seed(888)
# Cette fonction crée une chaine de Markov qui a pour loi stationnaire
# la loi Normale Centrée Réduite
#   x0 : état initial de la chaine
#   L : longeur de la chaine
#   s : paramètre
chaine_mcmc_N01 <- function(x0, L, s){
res <- rep(NA, L)
res[1] <- x0
for (l in 2:L){
x_old <- res[l-1]
x_new <- runif(1, min = x_old - 2 * s, max = x_old + 2 * s)    # générer un candidat
u_new <- runif(1)
rho <- pmin(1, dnorm(x_new) / dnorm(x_old))
if (u_new < rho){               # on accepte le nouveau candidat
res[l] <- x_new
}
else{                           # on conserve l'ancienne valeur
res[l] <- x_old
}
}
res
}
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 0.8)
plot(ts(a))
hist(a, breaks = 100)
set.seed(555)
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 0.8)
plot(ts(a))
hist(a, breaks = 100)
set.seed(555)
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 0.8)
hist(a, breaks = 100)
set.seed(5555)
# Exemple
a <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 0.8)
plot(ts(a))
hist(a, breaks = 100)
plot(ts(a))
plot(ts(a), main "Série temporelle")
plot(ts(a), main = "Série temporelle")
hist(a, breaks = 100)
hist(a, breaks = 100, main = "Histogramme")
library(coda)
a <- as.mcmc(a)
summary(a)
# La convergence vers des valeurs classiques d'une N(0,1) est rapide
plot(ts(Y), main = "Série temporelle")
set.seed(5555)
# Cette fonction crée une chaine de Markov qui a pour loi stationnaire
# la loi Normale Centrée Réduite
#   x0 : état initial de la chaine
#   L : longeur de la chaine
#   s : paramètre
chaine_mcmc_N01 <- function(x0, L, s){
res <- rep(NA, L)
res[1] <- x0
for (l in 2:L){
x_old <- res[l-1]
x_new <- runif(1, min = x_old - 2 * s, max = x_old + 2 * s)    # générer un candidat
u_new <- runif(1)
rho <- pmin(1, dnorm(x_new) / dnorm(x_old))
if (u_new < rho){               # on accepte le nouveau candidat
res[l] <- x_new
}
else{                           # on conserve l'ancienne valeur
res[l] <- x_old
}
}
res
}
# Exemple
# On part volontairement d'une valeur x0 éloignée de la moyenne
# La valeur du paramètre s doit être :
#   - assez grande pour converger rapidement
#   - mais pas trop pour éviter des effets plateau
Y <- chaine_mcmc_N01(x0 = -10, L = 1600, s = 0.8)
# La convergence vers des valeurs classiques d'une N(0,1) est rapide
plot(ts(Y), main = "Série temporelle")
# La distribution ressemble à celle d'une N(0,1)
hist(a, breaks = 100, main = "Histogramme")
library(coda)
Y <- as.mcmc(Y)
summary(Y)
traceplot(Y)
# La convergence vers des valeurs classiques d'une N(0,1) est rapide
plot(ts(Y), main = "Série temporelle")
traceplot(Y)
# Quantiles ergodiques
cumuplot(Y)
# Estimation de la densité
densplot(Y)
# Série temporelle
traceplot(Y)
# Si on retire 600 obs correspondant à la période de rodage de la chaine
Y <- Y[-(1:600),]
Y <- as.mcmc(Y)
# Si on retire 600 obs correspondant à la période de rodage de la chaine
Y <- Y[-(1:600)]
Y <- as.mcmc(Y)
# Quantiles ergodiques
cumuplot(Y)
# Estimation de la densité
densplot(Y)
autocorr.plot(X_bis)
autocorr.plot(Y)
summary(Y)
batchSE(Y)
batchSE(Y)
spectrum0(Y)
